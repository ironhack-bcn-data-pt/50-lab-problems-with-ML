{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Over & Underfitting\n",
    "## Predicting Boston Housing Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "In this project, you will use the Boston Housing Prices dataset to build several models to predict the prices of homes with particular qualities from the suburbs of Boston, MA.\n",
    "We will build models with several different parameters, which will change the goodness of fit for each. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Exploration\n",
    "Since we want to predict the value of houses, the **target variable**, `'MEDV'`, will be the variable we seek to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and explore the data. Clean the data for outliers and missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cnieto/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "boston_raw = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(boston_raw.data, columns=boston_raw.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MEDV'] = boston_raw.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "B          0\n",
       "LSTAT      0\n",
       "MEDV       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (12,7) \n",
    "sns.boxplot(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = IsolationForest( max_samples=100, random_state = 1, contamination= 'auto')\n",
    "preds = clf.fit_predict(df)\n",
    "df['Outliers_IsoFor'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "      <th>Outliers_IsoFor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.38799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.950</td>\n",
       "      <td>82.0</td>\n",
       "      <td>3.9900</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>232.60</td>\n",
       "      <td>27.71</td>\n",
       "      <td>13.2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.01360</td>\n",
       "      <td>75.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.410</td>\n",
       "      <td>5.888</td>\n",
       "      <td>47.6</td>\n",
       "      <td>7.3197</td>\n",
       "      <td>3.0</td>\n",
       "      <td>469.0</td>\n",
       "      <td>21.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.80</td>\n",
       "      <td>18.9</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.01311</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.403</td>\n",
       "      <td>7.249</td>\n",
       "      <td>21.9</td>\n",
       "      <td>8.6966</td>\n",
       "      <td>5.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>395.93</td>\n",
       "      <td>4.81</td>\n",
       "      <td>35.4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.15038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.581</td>\n",
       "      <td>5.856</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.9444</td>\n",
       "      <td>2.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>370.31</td>\n",
       "      <td>25.41</td>\n",
       "      <td>17.3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.38735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.581</td>\n",
       "      <td>5.613</td>\n",
       "      <td>95.6</td>\n",
       "      <td>1.7572</td>\n",
       "      <td>2.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>359.29</td>\n",
       "      <td>27.26</td>\n",
       "      <td>15.7</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>4.66883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.713</td>\n",
       "      <td>5.976</td>\n",
       "      <td>87.9</td>\n",
       "      <td>2.5806</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>10.48</td>\n",
       "      <td>19.01</td>\n",
       "      <td>12.7</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>8.20058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.713</td>\n",
       "      <td>5.936</td>\n",
       "      <td>80.3</td>\n",
       "      <td>2.7792</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>3.50</td>\n",
       "      <td>16.94</td>\n",
       "      <td>13.5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>3.77498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.655</td>\n",
       "      <td>5.952</td>\n",
       "      <td>84.7</td>\n",
       "      <td>2.8715</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>22.01</td>\n",
       "      <td>17.15</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.18337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.414</td>\n",
       "      <td>98.3</td>\n",
       "      <td>1.7554</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>344.05</td>\n",
       "      <td>23.97</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.20746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.093</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.8226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>318.43</td>\n",
       "      <td>29.68</td>\n",
       "      <td>8.1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS   RAD    TAX  \\\n",
       "32   1.38799   0.0   8.14   0.0  0.538  5.950  82.0  3.9900   4.0  307.0   \n",
       "54   0.01360  75.0   4.00   0.0  0.410  5.888  47.6  7.3197   3.0  469.0   \n",
       "55   0.01311  90.0   1.22   0.0  0.403  7.249  21.9  8.6966   5.0  226.0   \n",
       "123  0.15038   0.0  25.65   0.0  0.581  5.856  97.0  1.9444   2.0  188.0   \n",
       "126  0.38735   0.0  25.65   0.0  0.581  5.613  95.6  1.7572   2.0  188.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...   ...    ...   \n",
       "456  4.66883   0.0  18.10   0.0  0.713  5.976  87.9  2.5806  24.0  666.0   \n",
       "457  8.20058   0.0  18.10   0.0  0.713  5.936  80.3  2.7792  24.0  666.0   \n",
       "466  3.77498   0.0  18.10   0.0  0.655  5.952  84.7  2.8715  24.0  666.0   \n",
       "489  0.18337   0.0  27.74   0.0  0.609  5.414  98.3  1.7554   4.0  711.0   \n",
       "490  0.20746   0.0  27.74   0.0  0.609  5.093  98.0  1.8226   4.0  711.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  MEDV  Outliers_IsoFor  \n",
       "32      21.0  232.60  27.71  13.2               -1  \n",
       "54      21.1  396.90  14.80  18.9               -1  \n",
       "55      17.9  395.93   4.81  35.4               -1  \n",
       "123     19.1  370.31  25.41  17.3               -1  \n",
       "126     19.1  359.29  27.26  15.7               -1  \n",
       "..       ...     ...    ...   ...              ...  \n",
       "456     20.2   10.48  19.01  12.7               -1  \n",
       "457     20.2    3.50  16.94  13.5               -1  \n",
       "466     20.2   22.01  17.15  19.0               -1  \n",
       "489     20.1  344.05  23.97   7.0               -1  \n",
       "490     20.1  318.43  29.68   8.1               -1  \n",
       "\n",
       "[115 rows x 15 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Outliers_IsoFor == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.727272727272727"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(df[df.Outliers_IsoFor == -1]) / len(df)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LocalOutlierFactor in module sklearn.neighbors._lof:\n",
      "\n",
      "class LocalOutlierFactor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.OutlierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      " |  LocalOutlierFactor(n_neighbors=20, *, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, contamination='auto', novelty=False, n_jobs=None)\n",
      " |  \n",
      " |  Unsupervised Outlier Detection using the Local Outlier Factor (LOF).\n",
      " |  \n",
      " |  The anomaly score of each sample is called the Local Outlier Factor.\n",
      " |  It measures the local deviation of the density of a given sample with respect\n",
      " |  to its neighbors.\n",
      " |  It is local in that the anomaly score depends on how isolated the object\n",
      " |  is with respect to the surrounding neighborhood.\n",
      " |  More precisely, locality is given by k-nearest neighbors, whose distance\n",
      " |  is used to estimate the local density.\n",
      " |  By comparing the local density of a sample to the local densities of its\n",
      " |  neighbors, one can identify samples that have a substantially lower density\n",
      " |  than their neighbors. These are considered outliers.\n",
      " |  \n",
      " |  .. versionadded:: 0.19\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_neighbors : int, default=20\n",
      " |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      " |      If n_neighbors is larger than the number of samples provided,\n",
      " |      all samples will be used.\n",
      " |  \n",
      " |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      " |      Algorithm used to compute the nearest neighbors:\n",
      " |  \n",
      " |      - 'ball_tree' will use :class:`BallTree`\n",
      " |      - 'kd_tree' will use :class:`KDTree`\n",
      " |      - 'brute' will use a brute-force search.\n",
      " |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      " |        based on the values passed to :meth:`fit` method.\n",
      " |  \n",
      " |      Note: fitting on sparse input will override the setting of\n",
      " |      this parameter, using brute force.\n",
      " |  \n",
      " |  leaf_size : int, default=30\n",
      " |      Leaf is size passed to :class:`BallTree` or :class:`KDTree`. This can\n",
      " |      affect the speed of the construction and query, as well as the memory\n",
      " |      required to store the tree. The optimal value depends on the\n",
      " |      nature of the problem.\n",
      " |  \n",
      " |  metric : str or callable, default='minkowski'\n",
      " |      The metric is used for distance computation. Any metric from scikit-learn\n",
      " |      or scipy.spatial.distance can be used.\n",
      " |  \n",
      " |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      " |      must be square. X may be a sparse matrix, in which case only \"nonzero\"\n",
      " |      elements may be considered neighbors.\n",
      " |  \n",
      " |      If metric is a callable function, it is called on each\n",
      " |      pair of instances (rows) and the resulting value recorded. The callable\n",
      " |      should take two arrays as input and return one value indicating the\n",
      " |      distance between them. This works for Scipy's metrics, but is less\n",
      " |      efficient than passing the metric name as a string.\n",
      " |  \n",
      " |      Valid values for metric are:\n",
      " |  \n",
      " |      - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      " |        'manhattan']\n",
      " |  \n",
      " |      - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      " |        'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      " |        'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      " |        'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      " |        'yule']\n",
      " |  \n",
      " |      See the documentation for scipy.spatial.distance for details on these\n",
      " |      metrics:\n",
      " |      https://docs.scipy.org/doc/scipy/reference/spatial.distance.html.\n",
      " |  \n",
      " |  p : int, default=2\n",
      " |      Parameter for the Minkowski metric from\n",
      " |      :func:`sklearn.metrics.pairwise.pairwise_distances`. When p = 1, this\n",
      " |      is equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      " |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      " |  \n",
      " |  metric_params : dict, default=None\n",
      " |      Additional keyword arguments for the metric function.\n",
      " |  \n",
      " |  contamination : 'auto' or float, default='auto'\n",
      " |      The amount of contamination of the data set, i.e. the proportion\n",
      " |      of outliers in the data set. When fitting this is used to define the\n",
      " |      threshold on the scores of the samples.\n",
      " |  \n",
      " |      - if 'auto', the threshold is determined as in the\n",
      " |        original paper,\n",
      " |      - if a float, the contamination should be in the range (0, 0.5].\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``contamination`` changed from 0.1\n",
      " |         to ``'auto'``.\n",
      " |  \n",
      " |  novelty : bool, default=False\n",
      " |      By default, LocalOutlierFactor is only meant to be used for outlier\n",
      " |      detection (novelty=False). Set novelty to True if you want to use\n",
      " |      LocalOutlierFactor for novelty detection. In this case be aware that\n",
      " |      you should only use predict, decision_function and score_samples\n",
      " |      on new unseen data and not on the training set.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of parallel jobs to run for neighbors search.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  negative_outlier_factor_ : ndarray of shape (n_samples,)\n",
      " |      The opposite LOF of the training samples. The higher, the more normal.\n",
      " |      Inliers tend to have a LOF score close to 1\n",
      " |      (``negative_outlier_factor_`` close to -1), while outliers tend to have\n",
      " |      a larger LOF score.\n",
      " |  \n",
      " |      The local outlier factor (LOF) of a sample captures its\n",
      " |      supposed 'degree of abnormality'.\n",
      " |      It is the average of the ratio of the local reachability density of\n",
      " |      a sample and those of its k-nearest neighbors.\n",
      " |  \n",
      " |  n_neighbors_ : int\n",
      " |      The actual number of neighbors used for :meth:`kneighbors` queries.\n",
      " |  \n",
      " |  offset_ : float\n",
      " |      Offset used to obtain binary labels from the raw scores.\n",
      " |      Observations having a negative_outlier_factor smaller than `offset_`\n",
      " |      are detected as abnormal.\n",
      " |      The offset is set to -1.5 (inliers score around -1), except when a\n",
      " |      contamination parameter different than \"auto\" is provided. In that\n",
      " |      case, the offset is defined in such a way we obtain the expected\n",
      " |      number of outliers in training.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  effective_metric_ : str\n",
      " |      The effective metric used for the distance computation.\n",
      " |  \n",
      " |  effective_metric_params_ : dict\n",
      " |      The effective additional keyword arguments for the metric function.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_samples_fit_ : int\n",
      " |      It is the number of samples in the fitted data.\n",
      " |  \n",
      " |  See also\n",
      " |  ----------\n",
      " |  sklearn.svm.OneClassSVM: Unsupervised Outlier Detection using\n",
      " |      Support Vector Machine.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000, May).\n",
      " |         LOF: identifying density-based local outliers. In ACM sigmod record.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.neighbors import LocalOutlierFactor\n",
      " |  >>> X = [[-1.1], [0.2], [101.1], [0.3]]\n",
      " |  >>> clf = LocalOutlierFactor(n_neighbors=2)\n",
      " |  >>> clf.fit_predict(X)\n",
      " |  array([ 1,  1, -1,  1])\n",
      " |  >>> clf.negative_outlier_factor_\n",
      " |  array([ -0.9821...,  -1.0370..., -73.3697...,  -0.9821...])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LocalOutlierFactor\n",
      " |      sklearn.neighbors._base.KNeighborsMixin\n",
      " |      sklearn.base.OutlierMixin\n",
      " |      sklearn.neighbors._base.NeighborsBase\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_neighbors=20, *, algorithm='auto', leaf_size=30, metric='minkowski', p=2, metric_params=None, contamination='auto', novelty=False, n_jobs=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Shifted opposite of the Local Outlier Factor of X.\n",
      " |      \n",
      " |      Bigger is better, i.e. large values correspond to inliers.\n",
      " |      \n",
      " |      **Only available for novelty detection (when novelty is set to True).**\n",
      " |      The shift offset allows a zero threshold for being an outlier.\n",
      " |      The argument X is supposed to contain *new data*: if X contains a\n",
      " |      point from training, it considers the later in its own neighborhood.\n",
      " |      Also, the samples in X are not considered in the neighborhood of any\n",
      " |      point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The query sample or samples to compute the Local Outlier Factor\n",
      " |          w.r.t. the training samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      shifted_opposite_lof_scores : ndarray of shape (n_samples,)\n",
      " |          The shifted opposite of the Local Outlier Factor of each input\n",
      " |          samples. The lower, the more abnormal. Negative scores represent\n",
      " |          outliers, positive scores represent inliers.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit the local outlier factor detector from the training dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      " |          Training data.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present for API consistency by convention.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : LocalOutlierFactor\n",
      " |          The fitted local outlier factor detector.\n",
      " |  \n",
      " |  fit_predict(self, X, y=None)\n",
      " |      Fit the model to the training set X and return the labels.\n",
      " |      \n",
      " |      **Not available for novelty detection (when novelty is set to True).**\n",
      " |      Label is 1 for an inlier and -1 for an outlier according to the LOF\n",
      " |      score and the contamination parameter.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features), default=None\n",
      " |          The query sample or samples to compute the Local Outlier Factor\n",
      " |          w.r.t. to the training samples.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present for API consistency by convention.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      is_inlier : ndarray of shape (n_samples,)\n",
      " |          Returns -1 for anomalies/outliers and 1 for inliers.\n",
      " |  \n",
      " |  predict(self, X=None)\n",
      " |      Predict the labels (1 inlier, -1 outlier) of X according to LOF.\n",
      " |      \n",
      " |      **Only available for novelty detection (when novelty is set to True).**\n",
      " |      This method allows to generalize prediction to *new observations* (not\n",
      " |      in the training set).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The query sample or samples to compute the Local Outlier Factor\n",
      " |          w.r.t. to the training samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      is_inlier : ndarray of shape (n_samples,)\n",
      " |          Returns -1 for anomalies/outliers and +1 for inliers.\n",
      " |  \n",
      " |  score_samples(self, X)\n",
      " |      Opposite of the Local Outlier Factor of X.\n",
      " |      \n",
      " |      It is the opposite as bigger is better, i.e. large values correspond\n",
      " |      to inliers.\n",
      " |      \n",
      " |      **Only available for novelty detection (when novelty is set to True).**\n",
      " |      The argument X is supposed to contain *new data*: if X contains a\n",
      " |      point from training, it considers the later in its own neighborhood.\n",
      " |      Also, the samples in X are not considered in the neighborhood of any\n",
      " |      point.\n",
      " |      The score_samples on training data is available by considering the\n",
      " |      the ``negative_outlier_factor_`` attribute.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The query sample or samples to compute the Local Outlier Factor\n",
      " |          w.r.t. the training samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      opposite_lof_scores : ndarray of shape (n_samples,)\n",
      " |          The opposite of the Local Outlier Factor of each input samples.\n",
      " |          The lower, the more abnormal.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      " |  \n",
      " |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      " |      Find the K-neighbors of a point.\n",
      " |      \n",
      " |      Returns indices of and distances to the neighbors of each point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int, default=None\n",
      " |          Number of neighbors required for each sample. The default is the\n",
      " |          value passed to the constructor.\n",
      " |      \n",
      " |      return_distance : bool, default=True\n",
      " |          Whether or not to return the distances.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
      " |          Array representing the lengths to points, only present if\n",
      " |          return_distance=True.\n",
      " |      \n",
      " |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
      " |          Indices of the nearest points in the population matrix.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      In the following example, we construct a NearestNeighbors\n",
      " |      class from an array representing our data set and ask who's\n",
      " |      the closest point to [1,1,1]\n",
      " |      \n",
      " |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      " |      >>> neigh.fit(samples)\n",
      " |      NearestNeighbors(n_neighbors=1)\n",
      " |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
      " |      (array([[0.5]]), array([[2]]))\n",
      " |      \n",
      " |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      " |      element is at distance 0.5 and is the third element of samples\n",
      " |      (indexes start at 0). You can also query for multiple points:\n",
      " |      \n",
      " |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      " |      >>> neigh.kneighbors(X, return_distance=False)\n",
      " |      array([[1],\n",
      " |             [2]]...)\n",
      " |  \n",
      " |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      " |      Compute the (weighted) graph of k-Neighbors for points in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |          For ``metric='precomputed'`` the shape should be\n",
      " |          (n_queries, n_indexed). Otherwise the shape should be\n",
      " |          (n_queries, n_features).\n",
      " |      \n",
      " |      n_neighbors : int, default=None\n",
      " |          Number of neighbors for each sample. The default is the value\n",
      " |          passed to the constructor.\n",
      " |      \n",
      " |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      " |          Type of returned matrix: 'connectivity' will return the\n",
      " |          connectivity matrix with ones and zeros, in 'distance' the\n",
      " |          edges are distances between points, type of distance\n",
      " |          depends on the selected metric parameter in\n",
      " |          NearestNeighbors class.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      " |          `n_samples_fit` is the number of samples in the fitted data.\n",
      " |          `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n",
      " |          The matrix is of CSR format.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n",
      " |          of Neighbors for points in X.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> X = [[0], [3], [1]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      " |      >>> neigh.fit(X)\n",
      " |      NearestNeighbors(n_neighbors=2)\n",
      " |      >>> A = neigh.kneighbors_graph(X)\n",
      " |      >>> A.toarray()\n",
      " |      array([[1., 0., 1.],\n",
      " |             [0., 1., 1.],\n",
      " |             [1., 0., 1.]])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LocalOutlierFactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LocalOutlierFactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Outliers_Log'] = lof.fit_predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[(df.Outliers_Log == -1) & (df.Outliers_IsoFor == -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the end, we will combine the results of both outlier methods,\n",
    "# and drop only the rows that are labeled as such by both of them,\n",
    "# because detecting outliers individually lead us to drop too many rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_outliers = list(df[(df.Outliers_Log == -1) & (df.Outliers_IsoFor == -1)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(rows_outliers,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[(df.Outliers_Log == -1) & (df.Outliers_IsoFor == -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop the columns Outliers because we no longer need them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Outliers_Log','Outliers_IsoFor'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(487, 14)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we want to explore the data. Pick several varibables you think will be ost correlated with the prices of homes in Boston, and create plots that show the data dispersion as well as the regression line of best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your plots here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do these plots tell you about the relationships between these variables and the prices of homes in Boston? Are these the relationships you expected to see in these variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your response here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a heatmap of the remaining variables. Are there any variables that you did not consider that have very high correlations? What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your response here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Statistics\n",
    "Calculate descriptive statistics for housing price. Include the minimum, maximum, mean, median, and standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Developing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Define a Performance Metric\n",
    "What is the performance meteric with which you will determine the performance of your model? Create a function that calculates this performance metric, and then returns the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def performance_metric(y_true, y_predict):\n",
    "    \"\"\" Calculates and returns the performance score between \n",
    "        true and predicted values based on the metric chosen. \"\"\"\n",
    "    # Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Shuffle and Split Data\n",
    "Split the data into the testing and training datasets. Shuffle the data as well to remove any bias in selecting the traing and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Analyzing Model Performance\n",
    "Next, we are going to build a Random Forest Regressor, and test its performance with several different parameter settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves\n",
    "Lets build the different models. Set the max_depth parameter to 2, 4, 6, 8, and 10 respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Five separate RFR here with the given max depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot the score for each tree on the training set and on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Produce a plot with the score for the testing and training for the different max depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these results tell you about the effect of the depth of the trees on the performance of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your response here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Tradeoff\n",
    "When the model is trained with a maximum depth of 1, does the model suffer from high bias or from high variance? How about when the model is trained with a maximum depth of 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your response here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best-Guess Optimal Model\n",
    "What is the max_depth parameter that you think would optimize the model? Run your model and explain its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your response here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applicability\n",
    "*In a few sentences, discuss whether the constructed model should or should not be used in a real-world setting.*  \n",
    "**Hint:** Some questions to answering:\n",
    "- *How relevant today is data that was collected from 1978?*\n",
    "- *Are the features present in the data sufficient to describe a home?*\n",
    "- *Is the model robust enough to make consistent predictions?*\n",
    "- *Would data collected in an urban city like Boston be applicable in a rural city?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your response here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
